#+startup: Num
#+TITLE: Time Series: Defining a Search Engine
#+AUTHOR: Philipp Beer
#+EMAIL: beer.p@live.unic.ac.cy
#+OPTIONS: toc:nil
#+OPTIONS: num:3
#+LATEX_HEADER: \usepackage{parskip}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LATEX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LATEX_HEADER: \usepackage{dsfont}
#+LATEX_HEADER: \usepackage[margin=2.5cm]{geometry}
#+LATEX_HEADER: \usepackage[font=small, labelfont=bf, margin=1cm]{caption}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_CLASS_OPTIONS: [hidelinks,11pt]
#+PROPERTY: header-args :exports none :tangle "./bibliography/593_thesis.bib"
#+LATEX_HEADER: \usepackage[natbib=true,citestyle=ieee, maxcitenames=2, mincitenames=1]{biblatex} \DeclareFieldFormat{apacase}{#1} \addbibresource{./bibliography/593_thesis.bib}
#+LATEX: \newcommand{\compconj}[1]{\overline{#1}}
#+LATEX: \newcommand{\euler}{e}



* Purpose
The purpose of this thesis is to explore the possibility of creating a time series series search engine.

ts forecasting big challenge, stats approaches, ML approaches which lag - however, little explored area how curated datasets can improve forecasting results and possibly improve of data analytics results or just simply see which time series show similar properties. lots of work similarity measures but compute intensivenes forbids large scale use of them when large data amounts are present

explore a method here that provides value insight on similarity is flexible and computationally significantly cheaper than what is represents the current standard

* Introduction
Time series is often described as "anything that is observed sequentially over time" which usually are observed at regular intervals of time cite:hyndman2014forecasting. They can be described as collection of observations that are considered together in chronological order rather than as individual values or a multiset of values. Their representation can be described as ordered pairs:
$S = (s_1,s_2,\dots,s_n)$ where $s_n = (t_n,v_n)$. $t_n$ can be a date, timestamp or any other element that defines order. $v_1$ represents the observation at that position in the time series.

Time series are utilized to analyze and gain insight from historic events/patterns with respect to the observational variable(s) and their interactions. A second area of application is forecasting. Here time series are utilized to predict the observations that occur in future under the assumption that the historic information can provide insight into the behavior of the observed variables.

citeauthor:Fu_2011 in their work cite:Fu_2011 categorized time series research into (1) representation, (2) indexing, (3) similarity measure, (4) segmentation, (5) visualization and (6) mining. Research in these different fields started taking off in the second half of the 20^th century. For example in cite:_str_m_1969 the authors worked on questions of representation via sampling the sampling of time series in citeyear:_str_m_1969. All these different research areas always have to deal with the challenges that inhibit time series data. Generally datasets in this domain are large. Through this time series data incorporates the similar obstacles as high dimensional data, namely the "curse of dimensionality" cite:Tang_2019 and requiring large computational efforts in order to generate insights. And as will be discussed in [[sec:apps]] there are applications fields where vast amounts of time series are generated and a comparison between them is required.

In this thesis we will focus on creating a algorithm allowing the fast and meaningful comparison of an input time series or template against a vast array time series. Within the research many different areas and approaches have been attempted (at list here). However, there is a tendency to apply the simplest methods possible to achieve the desired results. For time series those are mainly Euclidean Distance and Dynamic Time Warping. While those methods will be explored in section [[sec:exis_work]] it can be said that those methods are simple, easy to understand and produce mostly reliable results in their respective application domains. Good performance is not their strong suit. Therefore, our approach is targeted to achieving comparable capability in identifying similar series, while achieving it with a significant reduction in computational complexity.

** Applications
<<sec:apps>>
Time series are encountered everywhere. Any metric that is captured over time can utilized as time series. Granularity can be used as descriptor for the sampling rate of a series or more general how often measurements for a particular metric are taken. This granularity has a tendency to increase as well. As example consumer electronics that capture health and fitness data can mentioned. Or sensors which are utilized in the automotive industry or heavy machinery where they are employed to capture information for predictive maintenance applications.\\

In the financial industry time series are a very fundamental component of decision making, like the development of stock prices over time or financial metrics of interest. The same is true for macro economic information or metrics concerning social structures in society, etc.\\

In the medical field time series are also ubiquitous. Whether they relate to patient date like blood pressure. The bio statistics field utilizes electro-graphical data like electrocardiography, electroencephalography and many others. In more aggregate medical analysis like toxicology analysis of drug treatments for vaccine approvals they are utilized and in many forms of risk management, for example, population level obesity levels.\\

In engineering fields the utilization is often times similar to the above but it also require that information that is captured in time series is transferred between locations in an efficient manner. For example voice calls are required to be transferred between the participants in a fast manner and with minimized levels of noise in the data. Another interesting industrial example in the biomedical technology field is Neuralink which aims to implement a brain-machine-interface (BMI) utilizing mobile hardware like smartphones as the basis for its computation. Here a large of amount of time series data is generated which requires quick processing to generate real-time information. citeauthor:Musk_2019 describes a recording system with 3072 electrodes generating time series data cite:Musk_2019 that is used to capture the brain information and visualized in real-time cite:Siegle_2017.

Time series data is paramount to a wide variety of areas, relating to many different fields. Looking at the trajectory it seems likely that going forward more time series data on a higher granularity will be generated. This in turn increases the need to be able process, analyze, compare and respond to the data with methods that are faster than today's standard options.


** Organization of this thesis
The rest of this thesis is organized as follows:

** TODO to be integrated
- refer to previous work on measures of similarity and outcome
- measure of similarity required
- challenges with time series (domains, granularity, length, outliers)
- area of signal processing interesting methods  
* Related work
<<sec:exis_work>>
Related work addressing the idea of time series search engine focuses on the system architecture and the data processing and pipelining aspect of this such an architecture cite:Zhang_2012. However, in citeyear:Keogh_2000 citeauthor:Keogh_2000 also applied a dimensionality reduction technique (Piecewise Constant Approximation) to execute fast search similarity search in  large time series databases. Other papers address domain specific questions like the introduction of a "Time-series Subimage Search Engine for archived astronomical data" cite:Kang_2021.

In order to be able to describe the closeness of time series or multiple time series to each a measure for similarity is required. In the literature various general measures and corresponding computation methods can be found. citeauthor:Wang_2012 reviewed time series measures and categorized the similarity measures into 4 categories: (1) lock-step measures, (2) elastic measures, (3) threshold-based measures, and (4) pattern-based measures. citeauthor:Zhang_2020 classify similarity measures in the categories: (1) time-rigid methods (Euclidean Distance), (2) time-flexible measures (dynamic time-warping), (3) feature-based measures (Fourier coefficients), and (4) model-based methods (auto-regression and moving average model) cite:Zhang_2020. Lock-step measures include the L_p-norms (Manhattan and Euclidean Distance) as well as Dissimilarity Measure (DISSIM). Elastic measures include metrics like Dynamic Time Warping (DTW) and edit distance based measures like Longest Common Subsequence (LCSS), Edit Sequence on Real Sequence (EDR), Swale and Edit Distance with Real Penalty. An example for threshold-based measures are threshold query based similarity search (TQuEST). And Spatial Assembling Distance (SpADe) is an example for pattern-based measures. In another paper, citeauthor:Gharghabi_2020 classify the space of similarity measures by the the most common measures into: (1) Euclidean Distance, (2) Dynamic Time Warping (DTW), (3) Least Common Subsequence (LCSS), and (4) K-Shape.

Dynamic Time Warping (DTW) is an elastic measure. It has been introduced by citeauthor:Berndt94usingdynamic in citeyear:Berndt94usingdynamic and its key advantage is the fact that comparison is applied on a one-to-many-basis allowing the comparison of regions from one series to regions of the other time series. This gives it the capability to warp peaks or valleys between different time steps of the two series as the resulting distance metric. As will be shown in section [[sec:dtw]] this comes at the price of time complexity which renders it effectively useless in practice when applied to large scale data sets.

Other attempts are also made in introducing new distance metrics. citeauthor:Gharghabi_2020 introduced a new metric called MPdist (Matrix Profile Distance) which is more robust than Euclidean Distance (ED) - more details can be found in section [[sec:ed]] - and Dynamic Time Warping (DTW) - more details can be found in section [[sec:dtw]] - and computationally preferable. Interestingly, due to the use of subsequences in the comparison of two time series its time complexity ranges from $\mathcal{O}(n^2)$ in the worst case, to $\mathcal{O}(n)$ in the best case and with this can provide a significant advantage of prevalent methods like ED or DTW.

The other research area of interest for our task is time series representation. It concerns itself with the optimal combination of reduction of the data dimensionality but adequate capture of its particular properties. With these methods feats like minimizing noise, managing outliers can be achieved. For many activities this is also the basis for the reduction of time complexity in the resulting algorithms that analyze and compare the time series.

According to citeauthor:Li_2019 the following methods are common methods for this task: (1) Discrete Fourier Transformation (DFT), (2) Singular Value Decomposition (SVD), (3) Discrete Wavelet Transformation (DWT), (4) Piecewise Aggregate Approximation (PAA), (5) Adaptive Piecewise Constant Approximation (APCA), (6) Chebyshev polynomials (CHEB), (7) Symbolic Aggregate approXimation, and others cite:Li_2019. In their paper, citeauthor:Pang_Liu_Peng_Peng_2018 mention (1) Singular Value Decomposition (SVD), (2) Frequency-Domain transformation, (3) Piecewise Linear Representation (PLR), (4) model-based method, and (5) symbolic representation.

** Dimensionality Reduction related to Singular Value Decomposition
Singular Value Decomposition is a fundamental matrix factorization technique with a plethora of applications and use cases. It's value comes from the capability of generating low rank approximations of data matrices that allow to represent the matrix values via the unitary matrices $\bm{U} \in \mathbb{C}^{n \times n}$ and $\bm{V} \in \mathbb{C}^{m \times m}$. The columns in in $\bm{U}$ and  $\bm{V}$ are orthonormal. The remaining matrix $\bm{\Sigma} \in \mathbb{R}^{n \times m}$, is a diagonal matrix with non-negative entries.

The power of the SVD is its ability to provide a low-dimensional approximation to high-dimensional data cite:brunton2019data. High dimensional data is often determined by a few dominant patterns which can be described by a low-dimensional attractor. Therefore, a prime application for the SVD is dimensionality reduction. It is complementary to the Fast Fourier Transform (FFT) which lays at the core of this work. citeauthor:brunton2019data describe it as the generalization of the FFT.

Principal Component Analysis (PCA) is a very common application of the SVD. It was developed by citeauthor:Pearson01 in citeyear:Pearson01. The main idea of PCA is to apply the SVD to a dataset centered around zero and subsequently computing the covariance of the centered dataset. Through the computation of the eigenvalues and their identifying the largest values the most important principal components are identified. Those are responsible for the largest variance in the dataset. And similar to the SVD their ranking and subsequent filtering can be used to focus on the most important components that allow to recreate majority of the of the variance in the dataset.

The Fast Fourier Transform (FFT) is based upon the Fourier Transform introduced by Joseph Fourier in early 19^th century to analyze and analytically represent heat transfer in solid objects cite:fourier1822th√©orie. This transform is a fundamental component of modern computing and science in general. It has transformed how technology can be used in the in 20^th century in areas such as image and audio compression and transfer. The concept will be introduced in more detail in section [[sec:fft]]. Its core idea is to represent the data to be transformed as the coefficients of a basis of sine and cosine eigenfunctions. It is similar to the principles of the SVD with the notable difference that the basis are an infinite sum of sine and cosine functions. The ability to reduce to the transformed data to few key components is the same as in SVD and PCA.

** Symbolic Aggregate approXimation
A dimensionality reduction technique that does not built on SVD and is geared directly towards time series is the Symbolic Aggregate approXimation (SAX) algorithm. Its core idea is to transform a time series into a set of strings via piecewise aggegrate approximation (PAA) and a conversion of the results via a lookup table cite:Lin_2003. Starting with PAA the reduction of a time series $T$ of length $n$ in vector $\bar{S} = \bar{s_1}, \bar{s_2}, \dots, \bar{s_w}$ of length $w$ where $w < n$, can be achieved through the following computation:
#+BEGIN_EXPORT latex
\begin{equation}
\bar{s_i} = \frac{w}{n} \sum_{j=\frac{n}{w}(i-1)+1}^{\frac{n}{w}i} s_j
\end{equation}
#+END_EXPORT

#+CAPTION: Piecewise Aggregate Approximation - M4 example: M31220 (window size - 6)
#+NAME: img_paa
[[./img/paa_example.png]]

This simply computes the mean of each of sub sequences determined through parameter $w$. An example from the M4 dataset can be seen in figure [[img_paa]]. For its application in SAX the time series are standardized or mean normalized, so that the comparison happens on the same amplitude. From this representation the data is further transformed to obtain a discrete representation via the mapping of the values computated via PAA to a symbolic representation of a letter. The used discretization should accomplish equiprobability in the assignments of the symbols cite:Lin_2007. The authors show by example of taking subsequences of length 128 from 8 different time series that the resulting PAA transformation has a Gaussian dstribution. This property does not hold for all series. And in place where it does not hold the algorithm performance deteriorates. If the assumption that the data distribution is Gaussian is true, breakpoints that will produce equal-sized areas can be obtained from a statistical table. The breakpoints are defined as $B = \beta_1, \beta_2, \dots, \beta_{a-1}$ so that the area under a Gaussian curve $N(0,1)$ from \beta_i to $\beta_{i+1}= \frac{1}{a}$  (\beta_0 and \beta_a are defined as -\inf and \inf) cite:Lin_2007. Table [[tab_breakpoints]] shows the value ranges for values of a from 3 to 10 and has been reproduced from cite:Lin_2007.

#+CAPTION: Lookup table - reproduced from citeauthor:Lin_2007
#+NAME: tab_breakpoints
| \beta_i |     3 |     4 |     5 |     6 |     7 |     8 |     9 |    10 |
|---------+-------+-------+-------+-------+-------+-------+-------+-------|
| \beta_1 | -0.43 | -0.67 | -0.84 | -0.97 | -1.07 | -1.15 | -1.22 | -1.29 |
| \beta_2 |  0.43 |     0 | -0.25 | -0.43 | -0.57 | -0.67 | -0.76 | -0.84 |
| \beta_3 |       |  0.67 |  0.25 |     0 | -0.18 | -0.32 | -0.43 | -0.52 |
| \beta_4 |       |       |  0.84 |  0.43 |  0.18 |     0 | -0.14 | -0.25 |
| \beta_5 |       |       |       |  0.97 |  0.57 |  0.32 |  0.14 |     0 |
| \beta_6 |       |       |       |       |  1.07 |  0.67 |  0.43 |  0.25 |
| \beta_7 |       |       |       |       |       |  1.15 |  0.76 |  0.52 |
| \beta_8 |       |       |       |       |       |       |  1.22 |  0.84 |
| \beta_9 |       |       |       |       |       |       |       |  1.28 |
|---------+-------+-------+-------+-------+-------+-------+-------+-------|

Based into which \beta category a value of PAA fits a symbol is assigned. "*a*" is reserved for values smaller than \beta_1 and values execeeding \beta_{a-1} is assigned the last symbolic value which differs depending on how many categories are chosen.

As stated before, this method relies on the fact that the data is normally distributed. Therefore, it can be great to detect for example anomalies in streaming data. Also the distance computation is preserved on the PAA values. However, the distance computation is still based on Euclidean Distance (ED) and has the same time complexity as before, but for fewer data points compared to the original series.
* Underlying Concepts
This section gives an overview of the concepts utilized in this thesis to generate the baseline performance of the algorithm against which our

** Euclidean Distance
<<sec:ed>>
Euclidean Distance is the most widely used distance metric in the research of time series. It is either used as a metric on its on or a as metric used used inside other methods to compute distances, for example, computation of distance of subsections of the data (cite:Faloutsos_1994) or to compute the distance between various points of two time series (see section [[sec:dtw]]). Having two time  series $S = \{s_1, s_2, \dots, s_n\}$ and $Q = \{q_1, q_2, \dots, q_n \}$ both of length $n$ the Euclidean distance can be computed as:
#+BEGIN_EXPORT latex
\begin{equation}
D(S,Q) = \sqrt{\sum_{i=1}^{n}{(S_i,Q_i)^2}}
\end{equation}
#+END_EXPORT

It is a measure that is easy to compute and comprehend and gives intuitive input for the distance of two time series. From the standpoint of time complexity the algorithm is applicable also to larger datasets with $\mathcal{O}(n)$. Its simplicity also creates some limitations. For example, to compute the euclidean distance between two series their length needs to be the same. Furthermore, it can be easily impacted in its results by the presence of outliers or increased levels of noise. It is not elastic with respect to the warping of information between two series in which effects that could indicate similarity happen even at slightly disparate steps. 

Despite its shortcomings it is a prominent metric and widely used for distance calculations for short comings. Some of its limitations are addressed by more sophisticated metrics that utilize its computation as component.

** Dynamic Time Warping
<<sec:dtw>>
citeauthor:Berndt94usingdynamic introduced Dynamic Time Warping in citeyear:Berndt94usingdynamic finding the minimal alignment between two time series computed through a cost matrix and identifying the minimized path through the matrix starting from the final elements of each time series. This warps the points in time between the different series as shown in figure [[img_dtw_example]].

#+CAPTION: Dynamic Time Warping - M4 Example: Y5683 and Y5376
#+NAME: img_dtw_example
[[./img/dtw_ex_plain.png]]

Two series $S = \{s_1, s_2, \dots, s_n\}$ of length $n$ and $Q = \{q_1, q_2, \dots, q_m\}$ of length $m$ are considered. For the series a n-by-m cost matrix $M$ is constructed. Each element in the matrix represents the respective i^th and j^th element of each of the two series which contains the distance of those to points:
#+BEGIN_EXPORT latex
\begin{equation}
m_{ij} = D(s_i, q_j)
\end{equation}
#+END_EXPORT

where often time euclidean distance is used as distance function $D(s_i, q_j) = (s_i - q_j)^2$. From the matrix a warping path $P$ is chosen, $P = p_1,p_2,\dots, p_k, \dots, p_K$ where:

#+BEGIN_EXPORT latex
\begin{equation}
\max(m,n) \leq k < m+n-1
\end{equation}
#+END_EXPORT

The warping path is constrained with bound with the following condition $p_1 = (1,1)$ and $p_K = (m,n)$. That means that both first elements of each series, as well as, the last element of each series are bound to each other in the computation. The warping path also is continuous. This means that from each chosen element $p_k$ only the neighboring elements to the left, right and diagonally can be chosen for the continuation of the path: $p_k= (a,b)$ and $p_{k-1} = (a',b')$ with $a-a' <=1$ and $b-b' <= 1$. The path elements $p_k$ are also monotonous, meaning that $a-a' \geq 0$ and $b-b' \geq 0$. From the resulting matrix considering the mentioned constraints a cumulative distance $\gamma(i,j)$ is computed recursively:
#+BEGIN_EXPORT latex
\begin{equation}
\gamma(i,j) = D(s_i,q_j) + \min \{\gamma(i-1, j-1), \gamma(i-1, j), \gamma(i, j-1)\}
\end{equation}
#+END_EXPORT
Therefore, the path can obtained by the following definition:
#+BEGIN_EXPORT latex
\begin{equation}
DTW(S,Q) = \min_{P: Warping Path}\left\{\sum_{k=1}^K \sqrt{p_k}\right\}
\end{equation}
#+END_EXPORT

Figure [[img_warp_path_ex]] provides an example for a warping path result.

#+CAPTION: Warping path example - M4 data: Y5683 and Y5376
#+NAME: img_warp_path_ex
[[./img/dtw_3way.png]]

The challenge with the application of DTW is the time complexity of the algorithm $\mathcal{O}(m*n)$ due to the fact that the distance compuation needs to be executed for each element of each series. Various methods for speed improvements have been introduced. The favorite principle was described by citeauthor:Ratanamahatana_2004. They introduced an adjustment window condition that where it is assumed that the optimal path does not drift very far from the diagonal of the cost matrix cite:Ratanamahatana_2004. However, this does not change the fundamental nature of the algorithm and computing DTW for multiple time series against a database of time series will require days of computation time even on modern computer architectures. 

In favor of DTW needs to be stated, that it is flexible with regards to the series used. The compared time series do not require to have the same length and can still be compared. This is a property that is not avaiable with Euclidean Distance. However, the user also needs to be aware of outliers in either data set which can lead to a clustering of the warping path or pathological matches around those extreme points in the series. 

Therefore in practice, Dynamic Time Warping is not a method suitable for comparing a single time series against a large array of series when speed is an important criterion as well as the handling of outliers in the dataset.

**** Similarity through decomposition
- introduce time series decomposition (reference in cite:hyndman2014forecasting)
- trend and seasonality (mention assumptions about period)
** Fast Fourier Transform
<<sec:fft>>
In Fourier analysis the Fast Fourier Transform (FFT) is a more efficient implementation of the Discrete Fourier Transform (DFT) that utilizes specific properties. The Discrete Fourier transform is based on the Fourier Transform (FT) which concerns itself with the representation of functions which in turn is built upon the Fourier series. We will give a brief introduction to them. However, a thorough introduction can be found in cite:brunton2019data. The principal idea Fourier analysis follows is that it can project functions - i.e. Fourier Transform - and data vectors - i.e. Discrete Fourier Transform - into a coordinate system defined by orthogonal functions (sine and cosine). To get the exact representation of a function or a data vector it has be done in infinitely many dimensions.
*** Inner Product of Functions and their norms
To get to the properties of of data under the Fourier transform we must start with the Hermitian inner product (cite:ratcliffe2006foundations) of functions in Hilbert spaces, $f(x)$ and $g(x)$ ($\compconj{g}$ denotes the complex conjugate of $g$) in the domain $x \in [a,b]$:
#+BEGIN_EXPORT latex
\begin{equation}
\langle f(x),g(x) \rangle = \int_a^b f(x) \, \compconj{g}(x)dx
\end{equation}
#+END_EXPORT
This means that the inner product of the functions $f(x)$ and $g(x)$ are equivalent to the integral between $a$ and $b$. This notion can be transferred to the vectors generated by these functions under discretization. We want to show that under the limit of data values $n$ of the functions $f(x)$ and $g(x)$ approaching infinity, $n \to \infty$ the inner product of the vectors approach the inner product of the functions. We take $\vec{f} = [f_1, f_2, \dots, f_n]^T$ and $\vec{g}= [g_1, g_2, \dots, g_n]^T$ and define the inner product as:
#+BEGIN_EXPORT latex
\begin{equation}
\langle\vec{f},\vec{g}\,\rangle = \sum_{k=1}^n f(x_k) \, \compconj{g}(x_k).
\end{equation}
#+END_EXPORT
This formula behaves as desired but grows in its value as more and more data points are added. So a normalization is added to counter the effect. The normalization occurs through the domain chosen for the analysis $\Delta x = \frac{b-a}{n-1}$:
#+begin_export latex
\begin{equation}
\frac{b-a}{n-1} \langle\vec{f},\vec{g}\, \rangle =\sum_{k=1}^n f(x_k) \, \vec{g}(x_k)\Delta x.
\end{equation}
#+end_export
This corresponds to the Riemann approximation of continuous functions cite:anton1998calculus. As more data more data points are collected and therefore $n \to \infty$ the inner product converges to the inner product of the underlying functions.

The norm of the inner product of the functions can also be expressed as integral:
#+begin_export latex
\begin{equation}
\|f\|_2 = (\langle f,\, f\rangle)^{\frac{1}{2}} = \sqrt{\langle f, \, f \rangle} = \left( \int_a^b f(x) \, \compconj{f}(x)dx  \right)^{\frac{1}{2}}.
\end{equation}
#+end_export
The last required step is transferring the applicability from a finite-dimensional vector space to an infinite-dimensional vector space. For this we can use the Lebesgue integrable functions or square integrable functions $L^2([a,b])$. All functions with a bounded norm define the set of square-integrable functions cite:brunton2019data. Next we will show how a Fourier series is a projection of a function onto the orthogonal set of sine and cosine functions.

*** Fourier Series
As the name suggests the Fourier series is an infinite sum of sine and cosine functions of increasing frequency. The mapped function is assumed to be periodic. A simple case of $2\pi$-periodic can be shown as:
#+begin_export latex
\begin{equation}
f(x) = \frac{a_0}{2} + \sum_{k=1}^\infty (a_k \cos(kx) + b_k\sin(kx)).
\end{equation}
#+end_export
If one imagines that this transformation projects the function onto a basis of cosine and sine, $a_k$ and $b_k$ are coefficients that represent the coordinates of where in that space the function is projected.
#+begin_export latex
\begin{equation}
a_0 = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)dx
\end{equation}
\begin{equation}
a_k=\frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \cos(kx)dx
\end{equation}
\begin{equation}
b_k=\frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \sin(kx)dx.
\end{equation}
#+end_export
Those coefficients are are acquired through integration and multiplication of sine and cosine.
This expression can be re-written in the form of an inner product:
#+begin_export latex
\begin{equation}
a_k = \frac{1}{\|\cos(kx)\|^2} \langle f(x),\, \cos(x)\rangle
\end{equation}
\begin{equation}
b_k = \frac{1}{\|\sin(kx)\|^2} \langle f(x),\, \sin(x)\rangle
\end{equation}
#+end_export
The squared norms are $\|\cos(kx)\|^2 = \|\sin(kx)\|^2 = \pi$. However, this only works for 2\pi-periodic functions. For real world data this is obviously most often not the case. Therefore, another term needs to be added that stretches the 2\pi-periodicity to length of the observed domain $[0,L)$ with $\frac{kx}{L}*2\pi$. This L-periodic function is then given by:
#+begin_export latex
\begin{equation}
f(x) = \frac{a_0}{2} + \sum \left( a_k\cos \left( \frac{2\pi kx}{L} \right) + b_k \sin \left( \frac{2\pi kx}{L}  \right)  \right)
\end{equation}
#+end_export
This modifies the integrals for the coefficients to:
#+begin_export latex
\begin{equation}
a_k = \frac{2}{L} \int_{0}{L} f(x) \cos \left( \frac{2\pi kx}{L}  \right)
\end{equation}
\begin{equation}
b_k = \frac{2}{L} \int_{0}{L} f(x) \sin \left( \frac{2\pi kx}{L}  \right)
\end{equation}
#+end_export
One can write the formula utilizing Euler's formula
#+begin_export latex
\begin{equation}
\euler^{ikx} = \cos(kx) + i \sin(kx),
\end{equation}
#+end_export
utilizing complex coefficients ($c_k = \alpha_k + i \beta_k$):
#+begin_export latex
\begin{equation}
\begin{aligned}
 f(x)={} & \sum_{k=- \infty}^{\infty} c_k \euler^{ikx} = \sum_{k=-\infty}^{\infty} (\alpha_k + i \beta_k) (\cos(kx) + i \sin(kx)) \\
 ={} & (\alpha_0 + i \beta_0) + \sum_{k=1}^{\infty} \left[ (a_{-k} + a_k) \cos(kx) + (\beta_{-k} - \beta_k) \sin(kx) \right] + \\
 & i \sum_{k=1}^{\infty} \left[ (\beta_{-k} + \beta_{k}) \cos(kx) - (\alpha_{-k}  - \alpha_k) \sin(kx)  \right].
\end{aligned}
\end{equation}
#+end_export
For real-valued functions it needs to be ensured that $c_{-k} = \compconj{c}_k$ through $\alpha_{-k}= \alpha_k$ and $\beta_{-k}= - \beta_k$. It also needs to be shown that theb basis provided by sine and cosine are orthogonal. This is only the case if both functions have the same frequency. We define $\psi_k = \euler^{ikx}$ for $k \in \mathcal{Z}$. This means that our sine and cosine functions can only take integer values as frequencies. To show that those are orthogonal over the interval $[0,2\pi)$ we look at the following inner product and equivalent integral:

#+begin_export latex
\begin{equation}
\langle \psi_j,\,\psi_k \rangle = \int_{-\pi}^{\pi} \euler^{jkx} \euler^{-ikx}dx =
\begin{dcases}
\mathrm{if} \, j \neq k & \int_{-\pi}^{\pi} \euler^{i0x} =  2\pi \\
\mathrm{if} \, j = k & \int_{-\pi}^{\pi} \euler^{i(j-k)x} =  0 \\
\end{dcases}
\end{equation}
#+end_export
When $j = k$ the integral reduces to 1, leaving $2\pi$ as the result of the interval to be integrated. In cas $j \neq k$ the expansion of the Euler's formula expression cancels out the cosine values and sine evaluated integer multiples of \pi is equal to $0$. Another way to express the inner product is via the Kronecker delta function:
#+begin_export latex
\begin{equation}
\langle \psi_j, \psi_k \rangle = 2\pi \delta_{jk}.
\end{equation}
#+end_export
This result can be transferred to a non-2\pi-periodic basis $e^{i2\pi \frac{kx}{L}}$ in $L^2 ([0,L))$. And the final step in the Fourier series is to show that any function f(x) is a projection on the infinitie orthognal-vector space  space that is spanned by cosine and sine functions:
#+begin_export latex
\begin{equation}
f(x) = \sum_{k=-\infty}^{\infty} c_k \psi_k(x) = \frac{1}{2\pi} \sum_{k=-\infty}^{\infty} \langle f(x),\,\psi_k(x)\rangle\psi_k(x).
\end{equation}
#+end_export
The factor $1/2\pi$ normalizes the projection by $\|\psi_k\|^2$.

*** Fourier Transform
So far, the Fourier series can only be applied to periodic functions. This means that after the length of the interval the function repeats itself. With the Fourier transform an integral is defined in which the domain goes to infinity in the limit such that functions can be defined without repeating itself. So if we define a Fourier series and its coefficients as:
#+begin_export latex
\begin{equation}
\begin{aligned}
f(x)={} & \frac{a_0}{2} + \sum_{k=1}^{\infty} \left[ a_k \cos\left( \frac{k\pi x}{L} \right) + b_k \sin \left( \frac{k\pi x}{L} \right)  \right] \\
= & \sum_{k=-\infty}^{\infty} c_k \euler^{\frac{ik\pi x}{L}}
\end{aligned}
\end{equation}
\begin{equation}
c_k = \frac{1}{2L} \langle f(x), \, \psi_k \rangle = \frac{1}{2L} \int_{-L}^{L} f(x)\euler^{- \frac{ik\pi x}{L}}dx.
\end{equation}
#+end_export
Our frequencies are defined by the $\omega_k = k\pi/L$. By taking a limit as $L \to \infty$ two properties are achieved:
1. the frequencies become a continuous range of frequencies
2. a infinite precision in the representation of our time series in the Fourier space is achieved.
We define $\omega_k = k\pi/L$ and $\Delta \omega_k = \pi /L$. As $L \to \infty$, $\Delta \omega \to 0$. We take the take the complex coefficient $c_k$ in its integral representation and apply the limit to $L$:
#+begin_export latex
\begin{equation}
f(x) = \lim_{\Delta \omega \to 0} \sum_{k=-\infty}^{\infty} \frac{\Delta \omega}{2\pi} \int_{-\frac{\pi}{\Delta \omega}}^{\frac{\pi}{\Delta \omega}} f(\xi)\euler^{-ik\Delta \omega \xi}d \xi \, e^{ik \Delta \omega x}.
\end{equation}
#+end_export
By taking the limit the inner product of the coefficient, i.e. the integral with respect to $\xi$ turns into the Fourier transform of $f(x)$ and the first part of the Fourier transform pair written as $\hat{f}$ and defined as, $\hat{f} \triangleq \mathcal{F}(f(x))$:
#+begin_export latex
\begin{equation}
\hat{f}(\omega) = \mathcal{F}(f(x)) = \int_{-\infty}^{\infty} f(x)\euler^{-i\omega x}dx
\end{equation}
#+end_export
The inverse Fourier transform utilizes $\hat{f}(\omega)$ to recover the original function $f(x)$:
#+begin_export latex
\begin{equation}
f(x) = \mathcal{F}^{-1}(\hat{f}(\omega)) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \hat{f}(\omega)\euler^{i\omega x}d\omega.
\end{equation}
#+end_export
As long as $f(x)$ and $\hat{f}(\omega)$ belong to the Lebesgue integrable functions the integrals converge. In effect this means that functions have to tend to 0 as $L$ goes to infinity.
*** Discrete Fourier Transform

In order to be able to apply the Fourier transform to time series a they need to be applicable to discrete data as well. The Discrete Fourier Transform (DFT) approximates the Fourier transform on discrete data $\textbf{f} = [f_1, f_2, \dots, f_n]^T$ where $f_j$ is regularly spaced.
The discrete Fourier transform pair is defined as:
#+begin_export latex
\begin{equation}
\hat{f}_k = \sum_{j=0}^{n-1} f_j\euler^{-2\pi jk/n},
\end{equation}
\begin{equation}
f_k = \frac{1}{n} \sum_{j=0}^{n-1}\hat{f}_j\euler^{i2\pi jk/n}.
\end{equation}
#+end_export
Via the DFT $\textbf{f}$ is mapped into the frequency domain $\hat{\textbf{f}}$. As before the output in the resulting DFT matrix is complex valued, meaning that it can be (and is) heavily used for physical interpretations for example in engineering questions as well.
*** Fast Fourier Transform

So far we have shown that the Fourier Series and the Discrete Fourier Transform can provide an exact representation of any arbitrary function or data generating process without requiring any assumptions or parameter settings. In the time complexity however we are dealing with an implementation that has complexity $\mathcal{O}(n^2)$. As an example, let's consider the M4 dataset, which will be introduced in section [[sec:m4_data]]. The longest series has $n=9919$ datapoints. Given the time complexity of the DFT this will include $\mathcal{O}(n^2)=9919^2=9.8 \times 10^8$ or about 1 billion operations. With the Fast Fourier Transform this can be reduced to a time complexity of $\mathcal{O}(n \log(n))$. In our example this results to $\mathcal{O}(9919 \log(9919)) = 1.3 \times 10^5$ or roughly 130,000 thousand operations. This is a improvement of factor 7,538. It is also an indication that when to time series it still provides very fast computation times.

To be able to convert the DFT to the FFT a multiple of 2 datapoints is required. For example, take $n=2^6=64$. In this case the DFT matrix can be written as follows:
#+begin_export latex
\begin{equation}
\hat{\textbf{f}} = \textbf{F}_{64}\textbf{f} =
\begin{bmatrix}
\textbf{I}_{32} & -\textbf{D}_{32} \\
\textbf{I}_{32} & -\textbf{D}_{32} \\
\end{bmatrix}
\begin{bmatrix}
\textbf{F}_{32} & \textbf{0} \\
\textbf{0} & \textbf{F}_{32} \\
\end{bmatrix}
\begin{bmatrix}
\textbf{f}_{\text{even}} \\
\textbf{f}_{\text{odd}}
\end{bmatrix},
\end{equation}
#+end_export
where $\textbf{I}_{32}$ is the Identity matrix. $\textbf{f}_{\text{even}}$ contain the even index elements of $\textbf{f}$, i.e. $\textbf{f}_{\text{even}} = [f_0, f_2,f_4, \dots, f_n]$ and $\textbf{f}_{\text{odd}}= [f_1,f_3,f_5, \dots, f_{n-1}]$. This process is executed recursively. In our example it would continue like this: $\textbf{F}_{32} \to \textbf{F}_{16} \to \textbf{F}_{8} \to \dots$ This is done down to $\textbf{F}_2$ where the resulting computations are excuted on $2 \times 2$ matrices, which is much more efficient than the DFT computations. Of course, it always has be broken down with the same process of taking the even and odd index rows of the resulting vectors. This significantly reduces the required computations to $\mathcal{O}= (n \log(n))$. Important is also that if a series does not have the length $n$ of a multiple of two, it is expedient to just pad the vector with zeros up to the length of the next power of two.
*** Parseval's Theorem
<<sec:parseval_thm>>
One property that the Fourier Transform has is central to the approach in this work. It is called Parseval's Theorem. It states that the integral of square of a function is equal to the integral of the square of its transform. In other words, the $L_2$-norm is preserved. This can be expressed as:
#+name: eq:pars_thm
#+begin_export latex
\begin{equation}
\int_{-\infty}^{\infty} \lvert \hat{f}(\omega) \rvert^2 d\omega = 2\pi \int_{-\infty}^{\infty} \lvert f(x) \rvert^2 dx.
\end{equation}
#+end_export

This property is important to us for multiple reasons. It tells us that angles and lengths are preserved in the frequency domain. This means, the different time series are comparable in the frequency domain they way they are in the time domain. And a second consequence that can be derived from this property is that frequencies with comparatively little power in the power spectrum (see section [[sec:pow_spec]]) can be removed from the representation in the frequency domain and still allow very similar reconstruction of the original time series. We will use this property in only comparing the top n most energetic frequencies of the all the frequencies computed in the Fourier transform (see section [[sec:freq_ranges]]).
*** Power Spectrum
<<sec:pow_spec>>
One important property of time series transformed is the resulting power spectrum or power spectral density (PSD). This concept comes from the signal processing field. The power spectrum denoted as $S_{xx}$ of a time series $f(t)$ describes the from which frequencies a signal is composed. It describes how the power of a sinusoidal signal is distributed over frequency. Even in the case of non-physical processes it is customary to describe it as power spectrum or the energy of a frequency per unit of time cite:press1992numerical.

To obtain the power spectrum we are converting our input vector via the FFT:
#+begin_export latex
\begin{equation}
\begin{bmatrix}
f_0 \\
f_1 \\
\vdots \\
f_n \\
\end{bmatrix}
\xrightarrow{FFT}
\begin{bmatrix}
\hat{f}_0 \\
\hat{f}_1 \\
\vdots \\
\hat{f}_n \\
\end{bmatrix}
\end{equation}
#+end_export
The resulting vector contains the complex values obtained through the FFT. We define the complex value contained in arbitrary value of the vector:
#+begin_export latex
\begin{equation}
\hat{f}_j \triangleq \lambda
\end{equation}
#+end_export
The complex value is represented as $\lambda = a + ib$. We compute the power of the particular frequency:
#+begin_export latex
\begin{equation}
\hat{f}_j = \lVert\lambda \rVert^2= \lambda \compconj{\lambda} = (a + ib)(a - ib) = a^2 + b^2.
\end{equation}
#+end_export
This is the magnitude of the particular frequency. In figure [[fig:fft_example]] an exemplartory time series from the M4 dataset (see section [[sec:m4_data]]) is visualized alongside the corresponding power spectrum of its Fourier Transform. The x-axis represents the corresponding frequencies obtained by the FFT, while the y-axis indidicates the energy contained in the respective frequencies. The x-axis is plotted in log-scale. An important property is the fact that the frequencies in the power spectrum differ depending on the length of the of the time series. A frequency of $k_a=2$ in a series $S_1$ length $n_{S_1}=5$ is equivalent to a frequency $k_b=4$ in a series $S_2$ of length $n_{S_2}= 10$.
#+CAPTION: Power Spectrum M4 - Example: M487
#+NAME: fig:fft_example
#+attr_latex: 200px
[[./img/fft_example.png]]
*** Spectral Leakage
<<sec:spec_leak>>
The Fast Fourier Transform (FFT) assumes that the signal continues infinitely in time and that there no discontinuities. However, any signal in the real world, including time series has finite data points. If the time domain is an integer multiple of of the frequency $k$ than each records connects smoothly to the next. Real world processes generally do not follow sinusoidal wave forms and can contain significant amounts of noise, as well as phase changes and trends. So if the signal is not an integer multiple of the sampling frequency $k$ this signal leaks into the adjacent frequency bins. See figure [[fig:fft_example]] in the power spectrum plot around 10^1. Both on the left a likely example of spectral leakage can be observed. As we intend to use the ranked by energy leve frequencies to look for similarities between time series this can be an issue as we want to avoid that the leaked frequencies are utilized for the determination of the most important frequencies. We will look at window functions to address this issue.
*** Window Functions
In the field of signal processing a lot of research has been conducted to combat the spectral leakage described in section [[sec:spec_leak]]. One way of addressing spectral leakage are window functions, also called tapering or apodization functions. They help reduce the undesired effects of spectral leakgage. They have been used successfully in various areas of signal processing, like speech processing, digital filter design and spectrum estimation cite:kumar2011. Spectrum estimation is the field we will apply them here.

The windows applied to data signals affect several properties of harmonic processors like the Fast Fourier Transform (FFT), like detectability, resolution, and others cite:harris1978. The window functions are designed such that in the spectral analysis they help reduce the side lobes next to the main beams of the spectral output of the Fast Fourier Transform (FFT). A side effect is that the main lobe broadens and thus the resolution is decreased cite:kumar2011. The sprectral power in a particular bin contains leakage from neighboring bins. The window function brings the data  down to zero at the edges of the time series. An example applied to a series from the M4 dataset can be seen in figure [[fig:ham_wdw]].

#+caption: Hamming window example with M4 time series M4516
#+name: fig:ham_wdw
[[./img/ham_window_example.png]]


The Hamming window is named after R.W. Hamming. It is one of many window functions and is defined as
#+begin_export latex
\begin{equation}
w(n) = 0.54 - 0.46 \cos \left( \frac{2\pi n}{M - 1}  \right) \quad
0 \leq n \leq M - 1,
\end{equation}
#+end_export
with $M$ being the length of time series to be covered. As can be seen in the figure, it minimizes the sidelobes created by the FFT, but it also minimizes valid signal at the edge of the time series data.

*** Bartlett's and Welch's Method
Another approach to address spectral leakage is to average periodograms generated over multiple subsets of the time series. Welch's method is based on Bartlett's method which is described in the following cite:bartlett1948. Let us denote the x^th periodgram or power spectrum as $\hat{P}$.
The idea that the average of the computed periodograms is unbiased:
#+begin_export latex
\begin{equation}
\lim_{N \to \infty} E\{\hat{P}_{per}(\euler^{j\omega})\} = P_x(\euler^{j\omega})
\end{equation}
#+end_export
So a consistent estimate of the mean, is a consistent estimate of the power spectrum. If we can assume that the realizations in the time series data are uncorrelated then they result in a consistent estimate of its mean. This means that the variance of the sample mean reduces with the number of measurements. They are inversly proportional. Therefore, averaging periodograms produces a the correct periodogram of the data. If we let $x_i(n)$ for $i = 1,2, \dots, K$ be $K$ uncorrelated realizations of a random process $x(n)$ over the interval of length $L$ with $ 0 \leq n < L$ and with $\hat{P}_{per}^{(i)}(\euler^{j\omega})$ the periodogram $x_i(n)$ is:
#+begin_export latex
\begin{equation}
\hat{P}_{per}^{(i)}(\euler^{j\omega})= \frac{1}{L} \left\lvert \sum_{n=0}^{L-1} x_i(n)\euler^{-jn\omega}  \right\lvert^2 \quad ; \quad
i= 1,2, \dots, K
\end{equation}
#+end_export
These periodograms can then be averaged
#+begin_export latex
\begin{equation}
\hat{P}_x (\euler^{j\omega}) = \frac{1}{K} \sum_{i=1}^K \hat{P}_{per}^{(i)}(\euler^{j\omega})
\end{equation}
#+end_export
and gives us an asymptotically unbiased estimate of the power spectrum. Because of the assumption that the values are uncorrelated, the variance is inversely proportional to the number of measurements K
#+begin_export latex
\begin{equation}
\text{Var} \left\{ \hat{P}_x(\euler^{j\omega})  \right\}= \frac{1}{K} \text{Var}\left\{ \hat{P}_{per}^{(i)}(e^{j\omega}) \right\} \approx \frac{1}{K}P_x^2(\euler^{j\omega})
\end{equation}
#+end_export
However, the assumption that the time series data is uncorrelated does not hold. Bartlett proposed to circumvent that to partition the data into $K$ non-overlapping sequences of length $L$ with a time series $X = \{x_1,x_2,\dots,x_n\}$ of length $N$ such that, $N = K \times L$.
#+begin_export latex
\begin{equation}
\begin{aligned}
x_i(n) = x(n + iL) \quad n = & 0,1,\dots,L-1 \\
                         i = & 0,1,\dots,K-1
\end{aligned}
\end{equation}
#+end_export
In consequence, Bartlett's method can be written as:
#+begin_export latex
\begin{equation}
\hat{P}_B(e^{j\omega}) = \frac{1}{N} \sum_{i=0}^{K-1} \left\lvert \sum_{n=0}^{L-1} x(n + iL)e^{-j\omega} \right\rvert^2
\end{equation}
#+end_export
An example of the split of time series can be seen in figure [[fig:bartlett]].
#+caption: Bartlett's window example from M4: D3720
#+name: fig:bartlett
[[./img/bartlett_example.png]]


Welch's method differs in how the windows are applied to the dataset. For Welch's method the windows are not adjacent are overlapping. The original data set is still split into $K$ sequences of length $L$ overlapping by $D$ points with $0 \leq D < 1$. If the overlapping is defined to be 0, then this method is equivalent to Bartlett's method. An overlap of 50% can be achieved via $D = K/2$. The overlapping of the data segments effectively cures the fact that an applied window minimizes the data at the edges of the window. The i^th sequence can be described by $x_i(n)= x(n+iD) \; ; \; n=0,1,\dots,L-1$ with $L$ being the length of a sequence. $N$ can be computed by $N = L + D(K-1)$ where $K$ is the number of sequences. Welch's method is described by
#+begin_export latex
\begin{equation}
\hat{P}_W(\euler^{j\omega})=\frac{1}{KLU} \sum_{i=0}^{K-1} \left\lvert \sum_{n=0}^{L-1} w(n)x(n+iD)\euler^{-jn\omega}  \right\rvert^2
\end{equation}
#+end_export
with
#+begin_export latex
\begin{equation}
U = \frac{1}{L} \sum_{n=0}^{L-1} \lvert w(n) \rvert^2
\end{equation}
#+end_export
An example of time series split via Welch's method with $K=4$ can be seen in figure [[fig:welch]].
#+caption: Welch's method windows example M4: D3720
#+name: fig:welch
[[./img/welch_example.png]]
* Time series representation

** Challenges when building a time series
- length of series
- trend
- seasonality
- time complexity -> issue because of data size
- granularity or sampling rates
- noise
- data quality
- similarity is task dependent (level)
- usual need for preprocessing the time series data (denoising, detrending, amplitude scaling) -> any pre-processing does modify the series

** Data Analysis
To develop a method to find similar time series in a pool of time series a replicatable data set is required that ideally represents real-world scenarios from a wide range of fields with differing time granularities. In the literature two widely used datasets can be found which will be introduced in sections [[sec:m4_data]] and [[sec:ucr_data]]. For the process of developing the FFT-based similarity detection method the M4 competition data was used cite:M4CompetitionArchive2018. All parameter choices were done with the exploratory data analysis results of the M4 data. To verify their veracity the formal evaluation of the method results were conducted with UCR Time Series Classification Archive cite:UCRArchive2018. This was done ensure that the results found and parameter choices made are applicable between different data domains and time granularities, as well as providing reference points for quality of the method described in this thesis.

*** M4 competition data
<<sec:m4_data>>
In his popular book citetitle:taleb2008black published in citeyear:taleb2008black the author citeauthor:taleb2008black introduced the M-competitions and its merits to an international readership. By that time already 3 M-competitions were already conducted with the first one done in citeyear:makridakis1982. citeauthor:makridakis1982 held the forecasting competition as a follow-up to a controversial paper published in citeyear:makridakis1979. In the paper citeauthor:makridakis1979 found that more sophisticated forecasting methods tended to lead to less accurate predictions, a view for which he highly criticited and personally attacked. The forecasting competition was an answer to the accusations to allow the experts to fine-tune their favorite forecasting methods to the best of their knowledge and compete for the most accurate predictions on the hold-out set cite:makridakis1982. The competition was based on 1001 different time series and provided an inside into the different properties of the various used forecasting methods. The data itself was selected with varying time granularities, different start and end times. It was chosen among data from different industries, firms and various countries. It consisted of macro-economic and micro-economic data. The results observed in the earlier work from citeyear:makridakis1979 was confirmed in the forecasting competition. The main observations were that stastically sophisticated methods on average provided not more accurate forecasts than simpler methods and accuracy improvement can be achieved by combining the results from various different methods cite:hyndman2020.

With the M4 a random selection of 100,000 series was performed by Professor Makridakis and provided for the forecasting competition in 2018. It included data with a time granularity ranging from hourly, daily, weekly, monthly, quarterly, and yearly data. It came from various areas: micro-economical, industrial, macro-economical, finance, demographic and miscellaneous areas cite:makridakis2020. This a wide field of mostly socio-economic data with varying time granularities, different time series length. What is not present or possibly underrepresented in the dataset are time series generated by technical processes, like machine or sensor data. Nonetheless, these time series data are an ideal candidate to develop and test and method for discovering similar time series. This time series archive was chosen as the dataset to develop the algorithm of indentifying similar time series quickly based on their Fast Fourier transform.

The latest completed  iteration of the Makridakis-competition is the M5 cite:spiliotis2021. It was completed in 2021 and was set up with product sales in 3 different states in 10 different stores in the United States. It consisted of the sales of 3490 different products sold by Walmart. The data came from the identical time frame ranging from 2011 to 2016. Due to the similar nature of the data contained in this dataset it was ruled as the basis for our investigation.

At the time of this writing in fall 2021, the next installment of the Makridakis-competition, the M6 is in planning to be conducted starting in February 2022.
*** UCR time series data
<<sec:ucr_data>>
Another important dataset with an even broader usage in time series research is the UCR Time Series Archive. It was first formed in 2002 by Prof. Keogh cite:hoang2019. It's intention was to provide a baseline for time series research which prior to that point mostly relied on testing a single time series per paper. The creators concluded that this makes comparing the results between papers almost impossible. The dataset was expanded in the subsequent decades with the last major expansion being conducted in 2018.

In his citeyear:keogh2003 published paper citeauthor:keogh2003 describe the error of data bias which comes from testing new methods on a single time series or time series of the same kind, for example ECG data but extending the claim of the found results to various types of time series data or all time series data types. With this in mind the UCR Time Series Archive was compiled and subsequently extended with various time series from various areas including: (1) Image, (2) Spectro, (3) Sensor, (4) Simulated, (5) Device, (6) Motion, (7) ECG, (8) Traffic, (9) EOG, (10) HRM, (11) Traffic, (12) EPG, (13) Hemodynamics, (14) Power, and (15) Spectrum time series data. This is a wide spectrum of data which is different from the socio-economic data of the Makridakis competition datasets. Therefore, this dataset is a great candidate to validate the findings of the time series similarity search and conduct a formal evaluation of the results found via the M4 dataset. Furthermore, it provides a classification category for each time series dataset which in itself is made up of multiple time series. In this way running a formal evaluation, we can measure how many datasets are identified between the train and test set of the data that belong to the same dataset and dataset class. This metric can then be compared between the Dynamic Time Warping (DTW) algorithm and our method.

** Challenges
- How many frequencies to compare?
- priorities of frequencies (power spectrum)
- different length of time series (leading to different  frequencies) - ranges solved with logs
* Methodology
** General Overview
The main idea of our method is to define the underlying frequencies as their most important property to identify similar time series. In a second step additional statistical metrics are used to reduce the number of similar series such that the user of the application can decide which for metrics the comparison should be executed.

The whole process consists of two general phases with further subdivisions of which only the second should be considered for computing the run-time of this method. Phase I is a  prepartory step required to set up the pool of time series which serve as the database from which the closest matches are identified. Phase I consists of:
1) Data Transformation (see section [[sec:data_trnf]])
2) Statistical Metrics Computation (see section [[sec:stat_mtr]])
 
Phase II describes how a single series considered as template series is matched against all available series in the database (see section [[sec:match_ts]]).

*** Data Transformation
<<sec:data_trnf>>
The preparation of the time series pool is done by executing the data transformation for all time series and computing the statistical metrics for all time series (section [[sec:stat_mtr]]). The data transformation is based on the Fast Fourier transform (FFT) and is executed multiple times for each series with multiple transformation types: (1) FFT with original data, (2) FFT with applied Hamming window on each time series, and (3) FFT with Welch's method and a Hamming window applied on each sub series for each time series. For a shorthand in the following "FFT" or "regular FFT" is used to describe the Fast Fourier transform without modification to the original data, "Hamming" is used to describe the FFT with a Hamming window applied to the original data, and "Welch" is used to describe the Fast Fourier transformation while applying Welch's method with a Hamming window on each subseries. The results from all three transformations are kept separately for later comparison to the template series.

After the transformations have been created only the top K (in our case top 5) frequencies, meaning the 5 frequencies with the highest magnitude in the frequency domain are retained and frequency range intervals are created (see section [[sec:freq_ranges]]). The top K frequencies are then associated with their respective frequency interval (see section [[sec:freq_assn]]). This process is depicted visually in figure [[fig:phase1a_fft]].

  #+caption: Phase 1a: convert time series pool to frequency space and identify top 5 frequency ranges
  #+name: fig:phase1a_fft
  [[./img/process_fft.png]]

With the completion of this step we have each time series associated with a list of K frequency intervals orded from lowest magnitude to highest magnitude associated with the respective series. So each time series is described by 5 data points irrespective of the length of the original series. Aside from other benefits this already hints at the fact that comparing 5 datapoints per comparison will be executed significantly faster than comparing hundreds or thousands of data points.

*** Statistical metrics computation
<<sec:stat_mtr>>
Describing a time series only by the top K frequency intervals in the Fourier domain is not sufficient to adequately describe the properties of a time series for matching it with other series. This, in part, is due to the fact that the magnitude of the particular frequency is not taken into account. In order to accomodate the possibility to use other well understood and common metrics we chose to compute additional statistical measures for the raw series and add them as additional datapoints describing the time series in the pool.

#+caption: Phase 1a: compute simple statistical metrics in time series pool for later comparison
#+name: fig:phase1b_stats
[[./img/process_simple_stats.png]]

As shown in figure [[fig:phase1b_stats]] the additional metrics are computed on the original time series, consisting of: (1) trend, (2) mean, (3) median, (4) standard deviation, (5) quantiles, and (6) minimum and max values. These metrics will be used flexibily to find similar series that match singular or multiple criteria. In essence the prior step of finding the underlying frequencies ensures that the time series follow similar periodicity or seasonality. The statistical metrics contain additional information that allow to find time series in the pool that, for example have similar value distribution through the standard deviation, etc and therefore match the users needs for the particular use case.

The trend mentioned above is not a strict statistical measure but the slope $m$ from the fitted linear equation:
#+begin_export latex
\begin{equation}
f(x) = m x + b
\end{equation}
#+end_export


Noteworthy is also the fact that the time complexity of the statistical metrics does not exceed $\mathcal{O}(n \log(n))$. It of course depends on the sorting algorithms used for the computation. Assumming quicksort or mergesort this holds true. This observation also includes the computation of the linear fit which is $\mathcal{O}(c^2 n)$ with $c$ representing the number of features which. For our case $c=1$, because we only have one feature or variable; hence time complexity for linear fit reduces to $\mathcal{O}(n)$. This observation let's us conclude that the computation for the statistical metrics will be feasible during the similarity search for the template time series even if $n$ is very large.
*** Matching of time series
<<sec:match_ts>>
After the completion of phase I the time series pool is ready for use. When a new time series is to be matched against the pool first phase I for the individual time series needs to be executed as well, meaning the data transformation into frequency space and computation of the statistical metrics. First, for each of the of the Fourier transform types (regular, Hamming, Welch) the highest matching score (see section [[sec:match_score]]) between the template time series $S_t$ and each of series in the pool $S_n$ is computed via:

#+begin_export latex
\begin{equation}
\argmax_{score\, \in \, S_{N}^{(type)}} f(score_{S_i}^{(type)}) = score_{S_i}^{(type)},
\end{equation}
#+end_export

where $type$ refers to the FFT type. This reduces the pool of the matching series to all time series from the pool per FFT type that are equivalent to the highest matching score for that transformation type. Next an additional limitation is applied that restricts the result set of matching series (named $A$) to the requirement of having a trend that must match in direction

#+begin_export latex
\begin{equation}
A_{trend} = \{S_{i} \in S_{N} \mid \mathds{1}\left( \frac{m_{S_t}}{-1} = \frac{m_{S_{i}}}{-1} \right)  \}
\end{equation}
#+end_export

One important component among the statistical metrics is the trend. It is found by utilizing a linear fit model. This metric in our algorithm is used to rule out time series from the pool that have an opposing trend from the result set. For example if the series for which we want to matching series in the pool has a negative trend, all series with a positive trend from the result set are ruled out before the other statistical metrics are utilized. However, if the trend for the investigation at hand is not relevant this step can easily be removed.



In our algorithm for each matching series based on the frequencies and having the same trend the delta of all other stastical metrics is computed and then ranked from smallest difference to highest difference. This step is executed without regard for the transform method used. The ranked difference between the template time series and the pool series is then used to select the most matching series
#+begin_export latex
\begin{equation}
\argmin_{S_i\, \in\, S_N} f(S_i):= \lvert\phi_{S_t} - \phi_{S_i}\rvert
\end{equation}
#+end_export


** Frequency Concepts
*** Frequency Ranges
<<sec:freq_ranges>>
We want to be able to compare the closeness of two time series by comparing their frequencies with each other. Due to Parseval's Theorem (see section [[sec:parseval_thm]]) we know that properties of the raw series are partially preserved in the frequency domain. Equation [[eq:pars_thm]] states that the energy contained in the norm of the frequency domain of the transformed time series is equal to the norm of $f(x)$. The energy in the norm of the transform is proportional to the norm of $f(x)$. What we can derive from that is that if there coefficients in the transform that are very small, they can be ignored without meaningfully impacting the result of the integral in the transform. Therefore, a truncated Fourier transform ranked by the magnitude of the coefficients will still remain a very good approximation to its original series. Additionally, because the Fourier transform is a unitary operator, meaning, it preserves lengths and angles in the frequency domain different series are comparable within in the Fourier space. So the distance between two time series is preserved in the frequency domain.

We utilize these properties by selecting the frequencies with the $n$ largest magnitudes for a comparison. We select multiple frequencies and rather than computing the distance between each of the same-ranked frequencies we want to assign them to a range band that can be used to capture whether two time series have frequencies at the same rank that matches within a certain bandwidth. This is an approximation of the distance as frequencies will be determined to be similiar up to a certain distance and then be declared not matching.

A second observation is that lower frequencies have a larger impact on the overall shape of a time series then higher frequencies. Therefore, a match at lower frequencies requires a smaller band than a match at higher frequencies. To accomodate this observation the range band is defined by the set defined on a logarithmic scale
#+begin_export latex
\begin{equation}
\Omega^{\prime}_n = \{ \omega^{\prime} = 10^\chi \in \mathbb{R} \mid \chi=k \cdot \Delta \; \land \; k \in \left[ \frac{a}{\Delta}, \frac{b}{\Delta}  \right], \quad
\Delta \in \mathbb{R_+}, \; k, a, b \in \mathbb{Z}\},
\end{equation}
#+end_export
where $\omega^{\prime}$ denotes the identified frequency range and  $\Delta$ is a fixed value defining the step size between the range intervals; $a$ and $b$ are the lower and upper limit of the interval($a < b$). Generally $k \ll a$ and $k$ must be an integer value to delineate the range borders. An example can be seen in figure [[fig:freq_example]]. For the figure a wider step change was chosen and the x-axis shown for both FFT and Hamming was limited to a smaller section so that the individual bins are and their associated values are visible.

#+caption: Frequency ranges definition - FFT example M4 data: M31291 with parameters $a=10^{-4}$ to $b=10^0$ with $=Delta=0.1$
#+name: fig:freq_example
[[./img/freq_range_example.png]]

For our work, we define $a=-4$, $b=0$ and $\Delta=0.01$.

*** Assigning frequencies to an interval
<<sec:freq_assn>>
The top K frequencies need to be assigned to their respective interval. The association is done via this mechanism:
#+begin_export latex
\begin{equation}
M_n(\omega)= n \mathds{1} \Omega^{\prime}_n(\omega) \quad \omega \in \left[\frac{a}{\Delta}, \frac{b}{\Delta}  \right].
\end{equation}
#+end_export
with $\omega$ representing one of the top K frequencies identified via the FFT and $\omega^{\prime}$ the respective representation in the frequency ranges set $\Omega^{\prime}$. As an example, imagine a frequency identified via the FFT of $\omega=0.003$ with $a=-3$,$b=0$, and $\Delta=0.1$. The value of $\omega$ falls into the interval $[10^{-2.6}, 10^{-2.5}]$. If $\Omega^{\prime}$ is indexed from 0, the result will be $M_n(\omega) = 6$.

*** Matching frequencies between time series and ranking results
<<sec:match_score>>
To match the frequencies between time series a mechanism is required that considers the rank of the match within the top K frequencies. We use the another logarithmic scale with base 10 to signify the importance of the match which can later be used for ranking the results with
#+begin_export latex
\begin{equation}
score = \sum_{k=0}^{K-1} 10^{k}\mathds{1}(\omega_{k}^{\prime \,(S_1)}) = \omega_k^{\prime \, (S_2)}
\end{equation}
#+end_export
where $\omega_{k}^{\prime \, (Sn)}$ represents the k^th ranked frequency band $\omega^{\prime}$ of time series $S_n$. The score is computed for each time series in the time series pool for each transform, meaning regular FFT, FFT with a Hamming window, and FFT with Welch's method using a Hamming window.

For each transform type all series are ranked based on their matching score in descending order. A higher score means that the more dominant frequencies in the series match. In the algorithm all time series from the pool that have the highest match score per transform type are selected for further processing that utilize the statistical metrics.
** Statistical metrics
To further reduce the number of matching series additional simple statistical metrics are used to identify the closest matches.
*** Investigated metrics
.

**** Forecasting
In the arena of forecasting the M-competition organized by Prof. Makridakis played a big role in the development of forecasting methods shortly after their inception in 1979.
# add paper and verify dates
One of the aspects that has been correct up until the 5th installment of the M-competition is that statistical methods in forecasting have outperformed more complex machine learning methods. So learning algorithms did not benefit sufficiently from learning from multiple series to generate more accurate point predictions and prediction intervals compared to the statistics-based alternatives.

One interesting question in this area is whether clustering of time series that have similar properties and training algorithms per cluster of "similar" series can help simplify the learning process for machine learning methods and in consequence improve their performance in future competitions.

# reference to relevant chapter
However, expressing similarity for time series is a challenging questions with respect to which metrics to utilize, time complexity as well as limiting assumptions that need to be made for time series.
*** Data Set Properties
#+caption: Histogram / KDE - M4 time series length
#+name: fig:m4_ts_len_hist
#+attr_latex: 200px
[[file:./img/hist_m4_ts_len.png]]

asdfasdfadsf
#+CAPTION: Histogram / KDE - UCR time series length
#+NAME: fig:ucr_ts_len_hist
#+attr_latex: 200px
[[file:./img/hist_ucr_ts_len.png]]

** Main contribution of the thesis
- transformation into Fourier-space
- transfer frequencies into frequency range band with increasing range width (using log scale)
- computation of frequency energy levels (sort and keep top 5) -> ask Prof. how to name this parameter
- conversion of ordered frequencies into frequency range band
- for each series to compare -> compare whether the frequency matches on the ordered positions -> provide exponential value per position -> match on more powerful frequencies is valued higher
** additional computations
- utilization of FFT utilizes only frequency space (future work should consider comparison of energy levels per frequency)
- additional simple statistics computed (mean, std, quantiles)
- ts decomposition for trend estimation (requires parameter for period) -> then best line fit for slope of the time series
- computation of deltas for each series to search with statistics and slope of all other time series (review computational complexlity)
- ranking of matching series based highest frequency range match and ONE statistic
**  Preprocessing
- M4 data wide format vs. long format
** Parallelization
- computation times
- scalability
- Samples for results only (stratification vs. non-stratification)
**** Threads vs. Processes

** Technology (check with Prof. if required)
R vs. Python vs. Mathematica, Matlab
- all languages have FFT either built in or available through common packages
** 
- load
- transform to FFT vector space
- compare most important frequencies
- compare candidates
- select winner (which criteria)
* Exploratory Data Study
- what do results look like

  #+CAPTION: Frequency Distribution for different FFT types
  #+NAME: fig:freq_dist_ucr
  [[./img/freq_dist_ucr.png]]
* Formal Evaluation
- (maybe ) improvement in forecasting approach
- find dataset with ground truth and compare DTW to this approach
- Distance metrics
- time complexity
* Conclusion & future work
** Successes
** Failures
** Flaws
- final computation
** What is missing
- denoising of time series
- adjustment of number of frequencies used
-
* Results & Discussion

#+LATEX: \printbibliography[title={References}, keyword={textref}]
#+LATEX: \printbibliography[title={Data References}, keyword={dataref}]

